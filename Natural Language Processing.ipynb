{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits of NLP\n",
    "\n",
    "As all of you know, there are millions of gigabytes every day are generated by blogs, social websites, and web pages.\n",
    "\n",
    "There are many companies gathering all of these data for understanding users and their passions and give these reports to the companies to adjust their plans.\n",
    "\n",
    "These data could show that the people of Brazil are happy with product A which could be a movie or anything while the people of the US are happy of product B. And this could be instant (real-time result). Like what search engines do, they give the appropriate results to the right people at the right time.\n",
    "\n",
    "You know what, search engines are not the only implementation of natural language processing (NLP) and there are a lot of awesome implementations out there.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Implimentations\n",
    "\n",
    "\n",
    "These are some of the successful implementation of Natural Language Processing (NLP):\n",
    "\n",
    "    Search engines like Google, Yahoo, etc. Google search engine understands that you are a tech guy so it shows you results related to you.\n",
    "    Social websites feeds like Facebook news feed. The news feed algorithm understands your interests using natural language processing and shows you related Ads and posts more likely than other posts.\n",
    "    Speech engines like Apple Siri.\n",
    "    Spam filters like Google spam filters. It’s not just about the usual spam filtering, now spam filters understand what’s inside the email content and see if it’s a spam or not.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Libraries\n",
    "\n",
    "There are many open source Natural Language Processing (NLP) libraries and these are some of them:\n",
    "\n",
    "    Natural language toolkit (NLTK).\n",
    "    Apache OpenNLP.\n",
    "    Stanford NLP suite.\n",
    "    Gate NLP library.\n",
    "\n",
    "Natural language toolkit (NLTK) is the most popular library for natural language processing (NLP) which was written in Python and has a big community behind it.\n",
    "\n",
    "NLTK also is very easy to learn, actually, it’s the easiest natural language processing (NLP) library that you’ll use.\n",
    "\n",
    "In this NLP Tutorial, we will use Python NLTK library."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#key Terminology\n",
    "\n",
    "    Tokenization – process of converting a text into tokens\n",
    "    Tokens – words or entities present in the text\n",
    "    Text object – a sentence or a phrase or a word or an article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Feature extraction is a technique to convert the content into the numerical vectors to perform machine learning.\n",
    "Text feature extraction\n",
    "Image feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "Bag of words is used to convert text data into numerical feature vectors with a fixed size.\n",
    "eg: text data\n",
    "1.Assign a fixed integer id to each word\n",
    "2.Number of occurrences of each word\n",
    "3.Store as the value feature\n",
    "4.Tokenizing\n",
    "5.Counting\n",
    "6.Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizerClass Signature\n",
    "class sklearn.feature_extraction.text.CountVectorizer\n",
    "(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
    "Class\n",
    "Built-in stopwordslist\n",
    "Overrides string tokenizer\n",
    "Specifies number of components to keep\n",
    "File name or sequence of strings\n",
    "Encoding used to decode the input\n",
    "Min Threshold\n",
    "Max Threshold\n",
    "Removes accents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Feature Extraction Considerations\n",
    "Sparse\n",
    "This utility deals with sparse matrix while storing them in memory. Sparse data is commonly noticed when it comes to extracting feature values, especially for large document datasets.\n",
    "Vectorizer\n",
    "It implements tokenization and occurrence. Words with minimum two letters get tokenized. We can use the analyzer function to vectorize the text data.\n",
    "Tf-idf\n",
    "It is a term weighing utility for term frequency and inverse document frequency. Term frequency indicates the frequency of a particular term in the document. Inverse document frequency is a factor which diminishes the weight of terms that occur frequently.\n",
    "Decoding\n",
    "This utility can decode text files if their encoding is specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "An important task in model training is to identify the right model for the given dataset. The choice of model completely depends on the type of dataset.\n",
    "Unsupervised\n",
    "Models identify patterns in the data and extract its structure. They are also used to group documents using clustering algorithms.\n",
    "Example: K-means\n",
    "Supervised\n",
    "Models predict the outcome of new observations and datasets, and classify documents based on the features and response of a given dataset.\n",
    "Example: Naïve Bayes, SVM, linear regression, K-NN neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search and Multiple Parameters\n",
    "Document classifiers can have many parameters and a Grid approach helps to search the best parameters for model training and predicting the outcome accurately\n",
    "\n",
    "In grid search mechanism, the whole dataset can be divided into multiple grids and a search can be run on entire grids or a combination of grids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "A pipeline is a combination of vectorizers, transformers, and model training.\n",
    "\n",
    "\n",
    "1. Vectorizer(Converts a collection of text documents into a numerical feature vector)\n",
    "\n",
    "\n",
    "2. Transformer(tf-idf)(Extracts features around the word of interest)\n",
    "\n",
    "3. Model Training\n",
    "(document classifiers)(Helps the model predict accurately)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the Bag-of-Words Model\n",
    "\n",
    "Let’s make the bag-of-words model concrete with a worked example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Collect Data\n",
    "\n",
    "Below is a snippet of the first few lines of text from the book “A Tale of Two Cities” by Charles Dickens, taken from Project Gutenberg.\n",
    "\n",
    "    It was the best of times,\n",
    "    it was the worst of times,\n",
    "    it was the age of wisdom,\n",
    "    it was the age of foolishness,\n",
    "\n",
    "For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2: Design the Vocabulary\n",
    "\n",
    "Now we can make a list of all of the words in our model vocabulary.\n",
    "\n",
    "The unique words here (ignoring case and punctuation) are:\n",
    "\n",
    "    “it”\n",
    "    “was”\n",
    "    “the”\n",
    "    “best”\n",
    "    “of”\n",
    "    “times”\n",
    "    “worst”\n",
    "    “age”\n",
    "    “wisdom”\n",
    "    “foolishness”\n",
    "\n",
    "That is a vocabulary of 10 words from a corpus containing 24 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3: Create Document Vectors\n",
    "\n",
    "The next step is to score the words in each document.\n",
    "\n",
    "The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.\n",
    "\n",
    "Because we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.\n",
    "\n",
    "The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
    "\n",
    "Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.\n",
    "\n",
    "The scoring of the document would look as follows:\n",
    "\n",
    "    “it” = 1\n",
    "    “was” = 1\n",
    "    “the” = 1   #It was the best of times,\n",
    "    “best” = 1\n",
    "    “of” = 1\n",
    "    “times” = 1\n",
    "    “worst” = 0\n",
    "    “age” = 0\n",
    "    “wisdom” = 0\n",
    "    “foolishness” = 0\n",
    "As a binary vector, this would look as follows:\n",
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "1\n",
    "\t\n",
    "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "The other three documents would look as follows:\n",
    "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
    "1\n",
    "2\n",
    "3\n",
    "\t\n",
    "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
    "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing Vocabulary\n",
    "\n",
    "As the vocabulary size increases, so does the vector representation of documents.\n",
    "\n",
    "In the previous example, the length of the document vector is equal to the number of known words.\n",
    "\n",
    "You can imagine that for a very large corpus, such as thousands of books, that the length of the vector might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary.\n",
    "\n",
    "This results in a vector with lots of zero scores, called a sparse vector or sparse representation.\n",
    "\n",
    "Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms.\n",
    "\n",
    "As such, there is pressure to decrease the size of the vocabulary when using a bag-of-words model.\n",
    "\n",
    "There are simple text cleaning techniques that can be used as a first step, such as:\n",
    "\n",
    "    Ignoring case\n",
    "    Ignoring punctuation\n",
    "    Ignoring frequent words that don’t contain much information, called stop words, like “a,” “of,” etc.\n",
    "    Fixing misspelled words.\n",
    "    Reducing words to their stem (e.g. “play” from “playing”) using stemming algorithms.\n",
    "\n",
    "A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document.\n",
    "\n",
    "In this approach, each word or token is called a “gram”. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams.\n",
    "\n",
    "    An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “please turn”, “turn your”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring Words\n",
    "\n",
    "Once a vocabulary has been chosen, the occurrence of words in example documents needs to be scored.\n",
    "\n",
    "In the worked example, we have already seen one very simple approach to scoring: a binary scoring of the presence or absence of words.\n",
    "\n",
    "Some additional simple scoring methods include:\n",
    "\n",
    "    Counts. Count the number of times each word appears in a document.\n",
    "    Frequencies. Calculate the frequency that each word appears in a document out of all the words in the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer\n",
    "\n",
    "CountVectorizer works on Terms Frequency, i.e. counting the occurrences of tokens and building a sparse matrix of documents x tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorizer\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency. TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "    Term Frequency (TF): is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n",
    "    \n",
    "    Inverse Document Frequency (IDF): is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and instantiate TfidfVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hotel': 2, 'food': 0, 'hote': 1, 'service': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"hotel food\",\"hote food service\"]\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(corpus)\n",
    "idf = vect.idf_\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict,bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
